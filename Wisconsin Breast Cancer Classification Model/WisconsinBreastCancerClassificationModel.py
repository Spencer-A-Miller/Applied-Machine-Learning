# -*- coding: utf-8 -*-
"""HWK1_Question_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mBch1mL4NsenQxQC4LaudCLKrAtSjr3i
"""

# Author: Spencer Miller
# Date: May 7th, 2023
# Description: Logistic Regression, Linear, and Gaussian model trained on the 
#              Breast Cancer Wisconsin (Diagnostic) Data Set sourced from the 
#              UC Irvine Machine Learning Repository.
#              This is a classification model which predicts 'begign' or 'malignant'
#              based on 30 features.
#              Performance evaluated using confusion matrix.
#
# More Source Information:
# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)

import numpy as np
import matplotlib.pyplot as plt

from sklearn import svm
from sklearn.metrics import confusion_matrix
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import precision_recall_curve
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support

# load wine data as an object
data = load_breast_cancer()

# print feature names
print(data.feature_names)

# print output labels
print(data.target_names)

# extract features and output labels

x = data.data # features
y = data.target # labels

##########################
#   Question 1 part i    #
##########################

# Logistic Regression is a classification algorithm used to predict a binary 
# outcome (either benign or malignant) based on a set of predictor variables. It is a 
# supervised learning algorithm that models the probability of the binary 
# response variable based on one or more predictor variables.

# Print the shapes of the input features and output target variable
print('Feature shape: ',x.shape)
print('Target shape: ',y.shape)

# Split data into training and test sets with a 80/20 split and a random 
# state of 0 for reproducibility
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)

# Print the shapes of the training and test sets
print('Training input shape: ',x_train.shape)
print('Training output shape: ',y_train.shape)
print('Test input shape: ',x_test.shape)
print('Test output shape: ',y_test.shape)

# Create a Logistic Regression model with solver set to 'liblinear' and 
# random state set to 0. 'liblinear' is limited to one-versus-rest schemes 
# and better suited for small datas smaller datasets
model = LogisticRegression(solver='liblinear', random_state=0)

# Train the model on the training data
model.fit(x_train, y_train)

# Generate predictions on the test data using the trained model
predictions_test = model.predict(x_test)

# Compute the confusion matrix for the model using the test data
cm = confusion_matrix(y_test, predictions_test, labels=model.classes_)

# Create a ConfusionMatrixDisplay object to display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)

# Plot the confusion matrix
disp.plot()
plt.show()

##########################
#   Question 1 part ii   #
##########################

# We are iterating through the range of 1 to 31 to create 30 models, 
# each with an incrementally increasing number of features. Then, training the 
# Logistic Regression model using the selected number of features and printing 
# the classification report.


# Arrays to store Precision and Recall for each model
precisions = []
recalls = []

# Loop for changing number of features
C = 1  #regularization parameter
for i in range (1,31):

    feat_lim = i  # Set the maximum number of features to use

    # Create a logistic regression model using the training data
    model = LogisticRegression(solver='liblinear', C=C, random_state=0).fit(x_train[:, :feat_lim], y_train)

    # Generate predictions on the test data using the model
    pred_test = model.predict(x_test[:, :feat_lim])

    # Print the classification report for the model, which includes precision, recall, and F1-score
    # The target_names argument specifies the labels of the two classes (in this case, 'malignant' and 'benign')
    print(f'Classification report for model with {feat_lim} features:\n{classification_report(y_test, pred_test, target_names=data.target_names)}\n')
    print(classification_report(y_test, pred_test, target_names=data.target_names))

    # Compute Precision-Recall curve
    proba = model.predict_proba(x_test[:, :feat_lim])
    precision, recall, _ = precision_recall_curve(y_test, proba[:, 1])
    precisions.append(precision)
    recalls.append(recall)

    # Generate the confusion matrix for the model and display it
    cm = confusion_matrix(y_test, pred_test)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()

    # Show the plot and print a separator
    plt.show()
    
    print('------------------------------------------------------')

##########################
#   Question 1 part iii  #
##########################

# This code is training and testing SVM models with linear kernel. 
# SVM stands for Support Vector Machines, which is a popular supervised machine 
# learning algorithm used for classification and regression tasks. The kernel 
# function is used to map the original input features into a higher dimensional 
# feature space, where it is easier to find a hyperplane that separates the 
# different classes of data points. The linear kernel is one of the simplest
# kernel functions, which assumes that the data is linearly separable.

from sklearn.svm import SVC

# print data shapes
print('Feature shape: ',x.shape)
print('Target shape: ',y.shape)

# Split data into training and test

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)

# print training and test shapes

print('Training input shape: ',x_train.shape)
print('Training output shape: ',y_train.shape)
print('Test input shape: ',x_test.shape)
print('Test output shape: ',y_test.shape)

# Define the kernel function for SVM with linear kernel
model_svm_linear = SVC(kernel='linear', C=C, probability=True).fit(x_train[:, :feat_lim], y_train)

# Train the SVM classifier on the training set
model.fit(x_train, y_train)

predictions_test = model.predict(x_test)
cm = confusion_matrix(y_test, predictions_test, labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)
disp.plot()
plt.show()

# Initializing two empty lists to store the precision and recall values for each model.
precisions = []
recalls = []

C = 1 #regularization parameter

# Create SVM classifier with a loop to train and test 30 
# SVM models with different numbers of features.
for i in range (1,31):
    feat_lim = i

    # Create a linear classification model
    model_svm_linear = svm.SVC(kernel='linear',C=C,probability=True).fit(x_train[:, :feat_lim],y_train)

    # Generate predictions on the test data using the model
    pred_test = model_svm_linear.predict(x_test[:, :feat_lim])

    # Print the classification report for the model, which includes precision, recall, and F1-score
    # The target_names argument specifies the labels of the two classes (in this case, 'malignant' and 'benign')
    print(f'Classification report for model with {feat_lim} features:\n{classification_report(y_test, pred_test, target_names=data.target_names)}\n')
    print(classification_report(y_test, pred_test, target_names=data.target_names))

    # Compute Precision-Recall curve
    proba = model_svm_linear.predict_proba(x_test[:, :feat_lim])
    precision, recall, _ = precision_recall_curve(y_test, proba[:, 1])
    precisions.append(precision)
    recalls.append(recall)

    # Generate the confusion matrix for the model and display it
    cm = confusion_matrix(y_test, pred_test)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()

    # Show the plot and print a separator
    plt.show()
    
    print('------------------------------------------------------')

##########################
#   Question 1 part iv  #
##########################

# This code is training and testing SVM models with gaussian kernel to model the
# decision boundary between two classes. The Gaussian kernel computes the dot 
# product of two data points after mapping them to a high-dimensional feature 
# space using a Gaussian function. The Gaussian function is a radial basis 
# function (RBF) that takes as input the Euclidean distance between two data 
# points and a scaling parameter, known as the bandwidth. The resulting kernel
# value decreases as the distance between the two points increases, creating a 
# smooth bell-shaped curve.

# The Gaussian kernel is a non-linear kernel that can handle non-linearly 
# separable data by mapping the data to a higher-dimensional space where it can
# become linearly separable. However, the use of a Gaussian kernel can lead to 
# overfitting when the bandwidth is too large, which causes the kernel to be 
# overly sensitive to small differences in the input data. On the other hand, 
# using a small bandwidth can result in underfitting, which makes the kernel 
# too smooth and unable to capture the underlying patterns in the data.


from sklearn.svm import SVC

# print data shapes
print('Feature shape: ',x.shape)
print('Target shape: ',y.shape)

# Split data into training and test

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)

# print training and test shapes

print('Training input shape: ',x_train.shape)
print('Training output shape: ',y_train.shape)
print('Test input shape: ',x_test.shape)
print('Test output shape: ',y_test.shape)

# Define the kernel function for SVM with Gaussian kernel
model_svm_gaussian = SVC(kernel='rbf', C=C, probability=True).fit(x_train[:, :feat_lim], y_train)

# Train the SVM classifier on the training set
model.fit(x_train, y_train)

predictions_test = model.predict(x_test)
cm = confusion_matrix(y_test, predictions_test, labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)
disp.plot()
plt.show()


precisions = []
recalls = []

C = 1 #regularization parameter

# Create SVM classifier with a loop to train and test 30 
# SVM models with different numbers of features.
for i in range (1,31):
    feat_lim = i

    # Create a model

    model_svm_gaussian = svm.SVC(kernel='rbf',C=C,probability=True).fit(x_train[:, :feat_lim],y_train)

    # Generate predictions on the test data using the model
    pred_test = model_svm_gaussian.predict(x_test[:, :feat_lim])

    # Print the classification report for the model, which includes precision, recall, and F1-score
    # The target_names argument specifies the labels of the two classes (in this case, 'malignant' and 'benign')
    print(f'Classification report for model with {feat_lim} features:\n{classification_report(y_test, pred_test, target_names=data.target_names)}\n')
    print(classification_report(y_test, pred_test, target_names=data.target_names))

    # Compute Precision-Recall curve
    proba = model_svm_gaussian.predict_proba(x_test[:, :feat_lim])
    precision, recall, _ = precision_recall_curve(y_test, proba[:, 1])
    precisions.append(precision)
    recalls.append(recall)

    # Generate the confusion matrix for the model and display it
    cm = confusion_matrix(y_test, pred_test)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()

    # Show the plot and print a separator
    plt.show()
    
    print('------------------------------------------------------')